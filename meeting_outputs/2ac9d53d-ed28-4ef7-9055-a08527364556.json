{
    "meeting_id": "2ac9d53d-ed28-4ef7-9055-a08527364556",
    "meeting_title": "Harward meeting",
    "timestamp": "2025-07-19T22:00:34.670432",
    "transcript": "Hello, everyone. My name is Tom and today I have with me two of my awesome colleagues and we can start by getting them introduced. Sameer. Hey, everyone. This is Sameer and I help in articulating the value provided by our open platform and the plethora of use cases that can be supported. Let's go to the next one. Hi, everyone. My name is Will and I talk about the how. How do we use the APIs? How do we integrate them? How do we deploy them go live in our production applications? Awesome. Thanks, guys. Today, the topic is about RingCentral's AI APIs around conversational intelligence. So let's get started. Question for Sameer. AI is a very highly used acronym over the past 45 years, especially. How do you understand it in context of conversations? What capabilities does it bring and how easy is it to use? Very well, Tom, you hit the nail on the head. Yes, AI is a very, very highly used acronym for last, I would say, five to six years. But when you think about AI in context of conversations, like the first thing that comes to the mind is speech to text capability or text to speech capabilities. But I think it's much, much beyond that. For example, you can do a lot of interaction analysis, sentiment analysis and emotion analysis with these APIs that we offer. It means if you are in a contact center kind of a use case where a supervisor needs to understand what kind of calls are coming in from all the people around the world to understand the sentiment level, they can simply have a pointer zero to five to figure out the calls coming in, fall in what range to understand the sentiment of those calls. That's one of the biggest, biggest powers that we give through our AI APIs. Wow, that is so interesting. Next question I have here is for Will. What are the different set of APIs that we support today? And how can you throw some light on the three of the top most used conversational intelligence API use cases across our portfolio of API products? Yeah, great question. We support basically APIs at two categories. One, they're audio APIs and the other set of APIs are based on text. And the three most popular APIs are you can say the speech to text because even audio gets converted into text and then the AI engine gives you the transcript of that. And it can give you transcript with punctuation such as comma so that you can turn it into a report. Another API very popular is the speaker diarheization API. So, for example, there are multiple people here in this meeting and the API will tell you which speaker is speaking at which point of time, who spoke what essentially. And then there is another API that does speaker identification. This is similar to speaker diarheization API. It detects who's speaking and if the AI model has been trained by the speaker's voice, it can tell you who the person is if you provide a label such as the name of the person. And then it will tell you, for example, Will is speaking at this time. Otherwise, it will just say person A, person B, person C. That's great, Will. Samir, Will, thank you so much again for explaining the whys and the hows of RingCentral's open platform and specifically our intelligent APIs. This program's in beta right now. We'll soon be working toward releasing it generally available to the public. So, please stay tuned and we look forward to getting some feedback from you.\n",
    "summary": "The Harward meeting involved Tom, Sameer, and Will discussing RingCentral's AI APIs around conversational intelligence. Sameer explained that AI in conversations goes beyond just speech-to-text or text-to-speech capabilities and includes interaction, sentiment, and emotion analysis. These features can be particularly useful in contact center scenarios to gauge sentiment levels of incoming calls. \n\nWill described the two categories of APIs supported: audio and text-based. He highlighted the three most popular APIs: speech-to-text, speaker diarization (identifying who is speaking when), and speaker identification (identifying who the speaker is if the AI model has been trained by their voice). \n\nThe key outcome of the meeting is that RingCentral's open platform and intelligent APIs are currently in beta testing, with plans for general public release soon. The team welcomes feedback on the program.",
    "action_items": [],
    "decisions": []
}