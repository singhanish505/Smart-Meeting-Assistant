{
    "meeting_id": "c4e81748-2a06-4ec9-9885-c286ccd38e86",
    "meeting_title": "Team Meeting",
    "timestamp": "2025-07-19T22:41:38.620856",
    "transcript": "Hello, everyone. My name is Tom and today I have with me two of my awesome colleagues and we can start by getting them introduced. Sameer. Hey, everyone. This is Sameer and I help in articulating the value provided by our open platform and the plethora of use cases that can be supported. Let's go to the next one. Hi, everyone. My name is Will and I talk about the how. How do we use the APIs? How do we integrate them? How do we deploy them go live in our production applications? Awesome. Thanks, guys. Today, the topic is about RingCentral's AI APIs around conversational intelligence. So let's get started. Question for Sameer. AI is a very highly used acronym over the past 45 years, especially. How do you understand it in context of conversations? What capabilities does it bring and how easy is it to use? Very well, Tom, you hit the nail on the head. Yes, AI is a very, very highly used acronym for last, I would say, five to six years. But when you think about AI in context of conversations, like the first thing that comes to the mind is speech to text capability or text to speech capabilities. But I think it's much, much beyond that. For example, you can do a lot of interaction analysis, sentiment analysis and emotion analysis with these APIs that we offer. It means if you are in a contact center kind of a use case where a supervisor needs to understand what kind of calls are coming in from all the people around the world to understand the sentiment level, they can simply have a pointer zero to five to figure out the calls coming in, fall in what range to understand the sentiment of those calls. That's one of the biggest, biggest powers that we give through our AI APIs. Wow, that is so interesting. Next question I have here is for Will. What are the different set of APIs that we support today? And how can you throw some light on the three of the top most used conversational intelligence API use cases across our portfolio of API products? Yeah, great question. We support basically APIs at two categories. One, they're audio APIs and the other set of APIs are based on text. And the three most popular APIs are you can say the speech to text because even audio gets converted into text and then the AI engine gives you the transcript of that. And it can give you transcript with punctuation such as comma so that you can turn it into a report. Another API very popular is the speaker diarheization API. So, for example, there are multiple people here in this meeting and the API will tell you which speaker is speaking at which point of time, who spoke what essentially. And then there is another API that does speaker identification. This is similar to speaker diarheization API. It detects who's speaking and if the AI model has been trained by the speaker's voice, it can tell you who the person is if you provide a label such as the name of the person. And then it will tell you, for example, Will is speaking at this time. Otherwise, it will just say person A, person B, person C. That's great, Will. Samir, Will, thank you so much again for explaining the whys and the hows of RingCentral's open platform and specifically our intelligent APIs. This program's in beta right now. We'll soon be working toward releasing it generally available to the public. So, please stay tuned and we look forward to getting some feedback from you.\n",
    "summary": "During the Team Meeting, Tom, Sameer, and Will discussed RingCentral's AI APIs around conversational intelligence. Sameer described the value provided by these APIs, including speech to text, interaction analysis, sentiment analysis, and emotion analysis. He highlighted their use in contact centers to understand the sentiment of incoming calls.\n\nWill listed two categories of APIs supported by the platform, audio and text. The three most popular APIs are speech to text, speaker diarization (identifying who is speaking at which point in time), and speaker identification (determining who the speaker is if the AI model has been trained with the speaker's voice).\n\nThe team concluded that the program is currently in beta testing and will soon be released to the public. Feedback from users is anticipated and encouraged.",
    "action_items": [],
    "decisions": []
}